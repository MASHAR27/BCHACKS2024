# -*- coding: utf-8 -*-
"""BCHacks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LWKBxpps4Bps0-Vfq580sMVvnYvQdFp4
"""

import torch
import numpy as np

# !pip install wandb
device = 'cuda' if torch.cuda.is_available() else 'cpu'

class Music(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.pos = torch.nn.Embedding(200, 64).to(device)
    self.projection = torch.nn.Linear(3, 64).to(device)
    self.trans = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(64, 16, batch_first=True), 16).to(device)
    self.output = torch.nn.Linear(64, 3).to(device)
  def forward(self, x):
    pos = self.pos(torch.arange(0,200).to(device))
    feature = self.projection(x) + pos
    feature = self.trans(feature, mask=torch.nn.Transformer.generate_square_subsequent_mask(200), is_causal=True)
    return self.output(feature)
model = Music()
model(torch.rand(1,200,3).to(device)).shape

with open("transformer.csv","r") as f:
  data=f.read().split("\n")

new_data=[]
for i in data:
  if len(i.split(","))>=200:
    new_data.append([[int(i) for i in j.split(" ")] for j in i.split(",")[:200]])
new_data = np.array(new_data)

new_data.shape

m = new_data.mean()
std = new_data.std()

m,std

new_data = (new_data)/std

import pylab
pylab.hist((new_data).flatten(), bins=np.arange(-0.5,0.5,0.01))

BATCH_SIZE = 256

import random
def getdata():
  data = random.choices(new_data, k=BATCH_SIZE)
  data = np.array(data)
  return np.concatenate((np.zeros((BATCH_SIZE,1,3)),data[:,:-1,:]), axis=1), data

np.concatenate((np.zeros((10,1,3)),new_data[:10,:-1,:]), axis=1).shape

getdata()[0].shape,getdata()[1].shape

loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)
scheduler =  torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, len(new_data)//BATCH_SIZE*10)

import wandb
wandb.login()

run = wandb.init(
    project="ba-hacks",
)

model = model.to(device)
runing_loss = 0
count = 0
for epoch in range(30):
  for i in range(len(new_data)//BATCH_SIZE):
    x, y = getdata()
    x, y = torch.tensor(x).to(device).to(torch.float32), torch.tensor(y).to(device).to(torch.float32)
    output = model(x)
    loss = loss_fn(output, y)
    loss.backward()
    runing_loss += loss.item()
    count += 1
    if count==200:
      wandb.log({"loss":runing_loss/200})
      runing_loss = 0
      count = 0
    optimizer.step()
    scheduler.step()